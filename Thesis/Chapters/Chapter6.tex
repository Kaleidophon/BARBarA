% Chapter 6

\chapter{Experiment $\mathcal{A}$: TransE für deutsche Wissensdaten} % Main chapter title

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter1}

%----------------------------------------------------------------------------------------

In diesem Kapitel soll der Ansatz von (\cite{bordes2013translating}) für deutsche Daten nachvollzogen werden.
Dabei wird erst erklärt, wie die Daten erstellt wurden. Danach wird die Idee zum Training der Daten weiter ausgeführt und
auf die neuen Datensätze angewendet, bevor schlussendlich eine Evaluation und eine Gegenüberstellung zu den Originalergebnissen
erfolgt.

\section{Datenerzeugung}

In (\cite{bordes2013translating}) werden mehrere Relationsdatensets erstellt, darunter eines namens \textsc{FB15k}. Dieses
besteht aus Relationstripeln der Form $(h, l, t)$\footnote{= $(head,\ link,\ tail)$}, wobei
$h$ und $t$ Entitäten und $l$ die verbindende Relation bezeichnen. Diese stammen aus \emph{Freebase}, einer
von einer Onlinegemeinschaft gepflegten Datenbank, in der mehr als 23 Millionen Entitäten durch Relationen miteinander verknüpft sind.
Mittlerweile ist die Seite offline; das Projekt wurde sukzessive in \emph{Wikidata}\footnote{Siehe \url{https://www.wikidata.org/wiki/Wikidata:Main_Page (zuletzt abgerufen am 20.05.16)}} integriert. Auch die
\emph{Freebase API}, die als Programmierschnittstelle zum Abfragen von Informationen dient, wird langsam abgeschaltet
\footnote{Siehe \url{https://en.wikipedia.org/wiki/Freebase (zuletzt abgerufen am (20.05.16))}}.\\

Die \textsc{FB15k}-Daten enthalten 592.213 Tripel mit 14.951 einzigartigen Entitäten und 1.345 einzigartigen Relationen.
In \emph{Freebase} sind Entitäten generell sprachlich unabhängig gehalten. So wird die Entität mit dem Kürzel \verb|/m/02vk52|
im Englischen mit dem Begriff \emph{World Bank} und im Deutschen mit \emph{Weltbank} bezeichnet. Somit ist auch
\textsc{FB15k} zumindest theoretisch mehrsprachig, da es nur die \textsc{Freebase}-Kürzel enthält.
Jedoch sind die Entitäten darin oft hauptsächlich im englischen bzw. amerikanischen Sprachraum relevante
Entitäten, die im deutschen Raum teils nicht sehr bekannt sind. Diese 1:1 ins Deutsche zu übernehmen gestaltet sich schwierig,
da nicht jedes Kürzel in \emph{Freebase} mit einer deutschen Bezeichnung aufgeführt ist. Das könnte jeweils drei Gründe haben:

\begin{enumerate}
  \item Es gibt keine deutsche Übersetzung
  \item Die Entität ist für den deutschen Sprachraum nicht relevant genug
  \item Bisher hat einfach kein Nutzer einen deutschen Begriff hinzugefügt
\end{enumerate}

Die Plausibilität dieser Gründe ist diskutabel: Nicht alle Begriffe brauchen eine Übersetzung, so sind beispielsweise
Personennamen i.d.R. durch alle Sprachen hinweg gleich. Schwieriger wird es bei Ortsnamen oder Namen von Organisationen:
Intuitiv drängt sich der Anschein auf, dass populäre Bezeichnungen eher übersetzt werden als unpopulärere
(\emph{United States of America} $\rightarrow$ \emph{Vereinigte Staaten von Amerika} \textbf{aber} \emph{University of Denver}
$\rightarrow$ \emph{University of Denver}).\\
Im Falle der Relevanz ist davon auszugehen, dass diese mit der Anzahl der Nutzer einher geht: Bei einer großen
Nutzerbasis ist davon auszugehen, dass diese primär Einträge von Entitäten bearbeiten, die im Kontext der Geschichte,
des Tagesgeschehens o. Ä. relevant sind. Gegeben genug Zeit und aktive Nutzer ist also anzunehmen, dass eine immer
größer werdende Prozentzahl von relevanten Entitäten an das Deutsche angepasst wird. Bedenkt man die Laufzeit von Freebase
seit 2007 (also zum Zeitpunkt des Schreibens dieser Arbeit rund 9 Jahre), so nehmen wir an, dass dieses Bedenken zwar nicht
ganz aus der Welt zu räumen, aber zumindest zu vernachlässigen ist.\\

Basierend auf dieser Argumentation werden korrespondierende ``deutsche'' Daten folgendermaßen erzeugt:
Es wird bei allen Relationstripeln eine Prüfung durchgeführt, ob beide Entitäten $h$ und $t$ eine deutsche Bezeichnung
besitzen. Falls nicht, wird dieses Tripel ausgelassen. Danach wird ggf. noch die inverse Relation ergänzt (diese wird
später beim Training benötigt), z.B. bei \emph{/location/location/contains} und \emph{/location/location/containedby}.

\begin{table}[h]
  \centering
  \def\arraystretch{1.5}
  \begin{tabular}{@{}llll@{}}
    \toprule
    \textsc{Datenset} & \textsc{\#Tripel} & \textsc{\#Entitäten} & \textsc{\#Relationen} \\
    \toprule
    FB15k & 592.213 & 14.951 & 1.345 \\
    GER14k & 459.724 & 14.334 & 1.236 \\
    \bottomrule
  \end{tabular}
  \caption[Daten über die Relationsdatensets FB15k und GER14k]{Daten über die Relationsdatensets FB15k und GER19k.
  Aufgelistet ist die Anzahl der Tripel (Datensätze), Entitäts- und Relationstypen.\label{fig:fb15kger14k}}
\end{table}

Somit gilt für die Menge englischer Relationstripel $S_{EN}$ und die Menge deutscher Tripel $S_{DE}$, dass letztere
eine echte Teilmenge ist: $S_{EN} \supsetneq S_{DE}$ \footnote{$\forall e \in S_{DE}: e \in S_{EN} \wedge S_{DE} \neq S_{EN}$}.
Verschiedene Informationen über die beiden Datensets sind in Abbildung \ref{fig:fb15kger14k} aufgelistet.

\section{Training}

Gegeben ist eine Menge von Relationstripeln $S$ der Form $(h, l, t) \in S$. Zusätzlich gibt es noch eine Entitätsmenge
$E$ sodass $h, t \in E$ und eine Menge von Relationen $L$ mit $l \in L$. Den Entitäten und Relationen werden zusätzlich
noch Vektoren zugewiesen, sodass $h, l, t \in \mathbb{R}^k$ gilt. Idealerweise gelten nach dem Training für ein gültiges Tripel $(h, l, t)$
dann $h + l \approx t$. Hinzu wird noch eine Menge aus korrumpierten bzw. ``falschen'' Tripeln $S'$ erstellt, indem
für jedes Tripel in $S$ entweder $h$ oder $t$ durch eine andere, zufällig gewählte Entität ersetzt wird:
\begin{equation}
  S' = \{(h', l, t) | h' \in E\} \cup \{(h, l, t') | t' \in E\}
\end{equation}

Diese Methode wird von (\cite{bordes2013translating}) rauschkontrastierendes Lernen (\emph{Noise-contrastive learning})
genannt.

Um die für das Training benötigte Verlustfunktion zu definieren, wird das Unähnlichkeitsmaß $d_p$ für ein Tripel bestimmt, welcher
entweder aus der $L_1$- oder der $L_2$-Norm abgeleitet wird, also:
\begin{equation}
    d_1(h + l, t) = \sum_{i=1}^k \| h_i + l_i - t_i \|
\end{equation}
\begin{equation}
    d_2(h + l, t) = \sqrt{\sum_{i=1}^k \| h_i + l_i - t_i \|^2}
\end{equation}

Zuletzt werden noch Tripel $s$ aus $S$ und zufällige Tripel $s'=(h', l, t')$ aus $S'$ in Pärchen in einem neuen Datenset $S^*$ gruppiert:
\begin{equation}
  S^* = \{(s, sample(S'))| s \in S\}
\end{equation}

Nun kann die Verlustfunktion $\mathcal{L}$ erstellt werden:
\begin{equation}\label{form:lossf}
  \mathcal{L} = \sum_{((h,l,t), (h', l, t')) \in S^*} max(0, \gamma + d_p(h + l, t) - d_p(h' + l, t'))
\end{equation}

Der Parameter $\gamma$ bezeichnet hier einen Hyperparameter, der dem System einen gewissen Spielraum lässt. Indem versucht
wird, den durch diese Funktion errechneten Verlust zu minimieren, wird beim Training sichergestellt, dass die Vektorrepräsentationen
von korrekten Tripeln die Gleichung $h + l \approx t$ bestmöglichst erfüllen und für korrumpierte Tripel möglichst weit
verfehlen.\\

Vor dem Training werden die Repräsentationen der Entitäten und Relationen mit einer uniformen Verteilung im Intervall
$[-\frac{6}{\sqrt{k}}, \frac{6}{\sqrt{k}}]$ initialisiert und im Falle von $l \in L$ zusätzlich normiert. Während jedem
Trainingsschritt wird für die aktuelle Repräsentation der Verlust berechnet und deren Werte mithilfe des
\emph{minibatch stochastic gradient descent}-Algorithmus angepasst. Die Minibatch enthält dabei ebenso viele gültige wie korrumpierte Tripel.
Das Training wird solange ausgeführt, bis die Fehlerrate auf dem Validationsset konvergiert\footnote{Konvergenz auf dem
Trainingsset könnte ein Zeichen für Overfitting sein.}.\\

Für das Reproduzieren der Ergebnisse wurde den Empfehlungen von (\cite{bordes2013translating}) gefolgt: So wird das Training
nach maximal 1000 Epochen beendet, außerdem gilt $k = 50$, $\gamma = 1$, $\lambda = 0,01$ und $d_1$ als Unähnlichkeitsmaß.

\section{Evaluation}\label{sec:transe-eval}

Um die erstellten Daten zu evaluieren, wurden bei den Relationen im Testset jeweils die Kopf- und Fußentitäten entfernt.
Danach wurden alle bekannten Entitäten aus $E$ eingesetzt und mithilfe des Unähnlichkeitsmaßes ansteigend sortiert. Der Rang
der korrekten (ursprünglich entfernten) Entität wurde dann gespeichert. Dieses Verfahren wurde zuerst von (\cite{bordes2011learning})
vorgeschlagen.\\

Daraus lassen sich zwei Evaluationsmaße ableiten: Den gemittelten Rang der korrekten Entität sowie den Anteil
der Evaluationsläufe, bei denen sich die richtige Entität innerhalb der Top 10 befand (Hits@10). Zusätzlich wurde noch
zwischen roh und gefiltert unterschieden: Bei letzterem werden mögliche Tripel ignoriert, die in dieser Form zufällig schon in
den Trainingsdaten vorkamen und so auf ``unfaire'' Art und Weise vor allen anderen Tripeln bevorzugt werden.
Als Baseline wurde eine Methode namens \emph{Unstructured} aus (\cite{bordes2013translating}) verwendet, bei denen
die Relationsvektoren einfach Nullvektoren entsprechen.
Die Ergebnisse für die auf FB15k und GER14k trainierten Daten sind in Abbildung \ref{fig:results1} festgehalten.

\begin{table}[h]
  \centering
  \def\arraystretch{1.5}
  \begin{tabular}{@{}lllll@{}}
    \toprule
     & \multicolumn{2}{c}{\textsc{\O\ Rang}} & \multicolumn{2}{c}{\textsc{Hits@10}} \\ \cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-5}
     \textsc{Datenset} & \emph{Roh} & \emph{Gefiltert} & \emph{Roh} & \emph{Gefiltert} \\
     \toprule
     \textsc{Unstr.} & & & & \\
     \textsc{FB15k} & 575,54 & \textbf{197,02} & \textbf{34,8} & \textbf{48,79} \\
     \textsc{GER14k} & \textbf{234,73} & 222,05 & \textbf{34,07} & 41,83 \\
     \bottomrule
  \end{tabular}
  \caption[Resultate für TransE mit \textsc{FB15k} und \textsc{GER14k}]{Resultate für TransE, trainiert auf dem
  \textsc{FB15k}- und dem \textsc{GER14k}-Datenset. Evaluiert gegen Unstructured (\textsc{Unstr.}), wobei einfach
  Nullvektoren als Relationsvektoren fungieren.\label{fig:results1}}
\end{table}

An den Ergebnissen ist zuerst zu erkennen, dass sie von den Ergebnissen in (\cite{bordes2013translating}) für FB15k etwas abweichen.
Dies könnte daran liegen, dass bei der Beschreibung des Trainingsverfahren angegeben wurde, dass das Prozedere
nicht unbedingt 1000 Epochen lang ausgeführt wurde. In dem Code, der von den Autoren online zur Verfügung gestellt und
zum Training verwendet wurde\footnote{\url{https://github.com/glorotxa/SME}}, wurde dazu leider keine entsprechende Option gefunden, weshalb vermutlich davon auszugehen
ist, dass dies manuell ausgeführt oder nicht im Code festgehalten wurde.
Daraus ist zu schließen, dass die hier aufgeführten Ergebnisse schlechter sind als die,
die im Paper angegeben wurden, weil die Vektorrepräsentationen zu sehr auf die Trainingsdaten angepasst wurden und dann schlecht
auf den Testdaten generalisieren. Da die Daten aus \textsc{GER14k} eine Untermenge von \textsc{FB15k} sind, sind deren Ergebnisse
i.d.R. gleichwertig oder schlechter. Die Frage, warum der rohe gemittelte Rang bei \textsc{GER14k} signifikant besser ist als
bei FB15k, könnte dahingehend beantwortet werden, dass auch hier eine übermäßige Anpassung auf die Trainingsdaten stattfand und
zuviele Trainingstripel vor der eigentlich richtigen Lösung rangieren. Deshalb ist der Wert für die gefilterten Tripel umso
besser.

\section{Zwischenfazit}

In diesem Kapitel wurde ein Verfahren vorgestellt, das es erlaubt, Vektorrepräsentationen zu trainieren, die Entitäten und Relationen
aus Wissensdatenbanken modellieren und zur Relationsvorhersage genutzt werden können. Dabei bestehen die Daten aus Relationstripeln,
also 3-Tupeln mit einer Kopf- und einer Fußentität und einer dazugehörigen semantischen Relation. Es werden außerdem noch
Negativbeispiele erzeugt, indem eine der beiden Entitäten durch eine andere in den Trainingsdaten vorkommende Entität zufällig ausgetauscht wird.
Der Trainingsvorgang wird hierbei mithilfe eines auf Vektorarithmetik basierenden Unähnlichkeitsmaß betrieben, mit der
die ``Stimmigkeit'' korrekter Tripel versucht wird zu maximieren. Das Gegenteil gilt für korrumpierte Tripel.\\
Es wurde gezeigt, dass sich die in (\cite{bordes2011learning}) präsentierten Ergebnisse im Rahmen der gegebenen Möglichkeiten
annähernd replizieren lassen. Darüber hinaus wurde die Methodik auch auf eine Menge deutscher Daten angewendet, die eine
echte Teilmenge der englischen Daten darstellt. Für diese konnten ähnlich gute Ergebnisse erziehlt werden, wobei die qua
definitionem kleinere Menge an Übungsbeispielen dem Unterschied in der Performanz begründet.\\

Bei dieser Vorgehensweise wurden die Vektorrepräsentationen für Entitäten und Relationen gemeinsam trainiert. In den folgenden
Kapiteln soll ein Versuch unternommen werden, mithilfe von distributioneller Semantik erstellte Wortvektorrepräsentationen
für die Entitäten zu nutzen und darauf aufbauend für ähnliche Zwecke wie in diesem Kapitel zu nutzen.
