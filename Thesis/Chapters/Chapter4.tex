% Chapter 4

\chapter{Vorbereitung} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1}

%----------------------------------------------------------------------------------------

\section{Vorbereitung}

  Für die nachfolgenden Experimente in den Kapiteln \ref{Chapter6}, \ref{Chapter7} und \ref{Chapter8}
  mussten einige Daten zuerst beschafft und unter Umständen aufbereitet werden. Die  dazu ergriffenen Maßnahmen werden
  deshalb in diesem Kapitel beschrieben.

  \subsection{Verwendete Daten}

  Die anfangs verwendeten Daten lassen sich grob in vier Kategorien einteilen: Das Korpus, das Relationsdatenset,
  Wortähnlichkeitslisten sowie Analogiedatensets (siehe Abbildung \ref{fig:datasets}).\\
  Als Korpus wurde das \textsc{DECOW14X}-Korpus verwendet. Genaueres über dessen Beschaffenheit und Vorverarbeitung für das
  Training von Wortvektoren werden in \ref{sec:corpusprep} und \ref{sec:vectrain} beschrieben.

  \begin{table}[h]
    \centering
    \def\arraystretch{1.5}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llrll@{}}
      \toprule[1.5pt]
      \textsc{Art} & \textsc{Name} & \multicolumn{2}{l}{\textsc{Beschreibung}} & \textsc{Quelle} \\
      \toprule
      Korpora & \textsc{DECOW14X} & \multicolumn{2}{l}{Deutschsprachiges Webkorpus} & (\cite{schafer2012building}) \\
      %\midrule
      Relationsdaten & \textsc{FB14k} & \multicolumn{2}{l}{592k Relationstripel aus Freebase} & (\cite{bordes2013translating}) \\
      \midrule
      \multirow{4}{*}{\parbox[t]{1.5cm}{Wortähnlich-keiten}} & \textsc{Schm280} & 280 & \multirow{4}{*}{\Vast \} \parbox[c]{4cm}{nach Ähnlichkeit bewertete Wortpaare}} & (\cite{koper2015multilingual})\\ %\cline{5-5}
       & \textsc{Wortpaare65} & 65 & & (\cite{rubenstein1965contextual}) \\
       & \textsc{Wortpaare222} & 222 & & \\
       & \textsc{Wortpaare350} & 350 & & \\
      \midrule
      \multirow{2}{*}{Analogien} & \textsc{Google} & \multicolumn{2}{l}{Datenset mit 18.552 Analogien} & (\cite{koper2015multilingual}) \\
       & \textsc{SemRel} & \multicolumn{2}{l}{Datenset mit 2.462 Analogien} & \\
       \bottomrule[1.25pt]
    \end{tabular}%
    }
  \caption[Übersicht über verwendete vorgefertigte Datensets]{Übersicht über in dieser Arbeit verwendete, extern generierte
  Datensets sowie deren Quelle.\label{fig:datasets}}
\end{table}

  Relationsdaten aus der mittlerweile in \text{Wikidata} integrierten Wissensdatenbank\footnote{Siehe
  \url{https://plus.google.com/109936836907132434202/posts/bu3z2wVqcQc} (zuletzt abgerufen am 21.07.16).}
   \textsc{Freebase} wurden im Datenset \textsc{FB14k} von (\cite{bordes2013translating}) gesammelt.
  Diese werden vor allem in den Versuchen von Kapiteln \ref{Chapter6} und \ref{Chapter8} relevant.\\

  Alle anderen Datensets sind zur Evaluation der Wortvektoren in Kapitel \ref{sec:we-eval} angedacht: Die Sets
  \textsc{Schm280}, \textsc{Wortpaare65}, \textsc{Wortpaare222} und \textsc{Wortpaare350} bestehen aus einer Reihe
  von Wortpaaren, die von menschlichen Annotatoren auf verschiedenen Skalen nach Ähnlichkeit bewertet wurden.\\
  \textsc{Google}\footnote{Eigentlich: \textsc{Google semantic/syntactic analogy datasets}} und \textsc{SemRel} sind
  in verschiedene Kategorien eingeteilte Analogiepaare der Form ``W verhält sich zu X wie Y zu Z''. Bei der Evaluation
  wird dann die Fähigkeit eines Systems mit Wortvektoren getestet, diese korrekt zu vorvervollständigen, wenn Z nicht
  gegeben ist.


  \subsection{Aufbereitung des Korpus}\label{sec:corpusprep}

  Als Textressource wurde das DECOW14X-Korpus (DE = Deutsch, COW = ``\textbf{CO}rpus from the \textbf{W}eb'') verwendet.
  Dieses Korpus von (\cite{schafer2012building}) besteht aus 21 Teilkorpora,
  die in den Jahren 2011 und 2014 von deutschsprachigen Internetseiten extrahiert und aufbereitet wurden. Diese beinhalten
  Informationen im Bezug auf PoS-Tagging, Chunking, Lemmatisierung, Eigennamen (\emph{named entities}) und einige Metadaten.
  Die Sätze liegen im \textsc{CoNLL}-Format\footnote{Siehe \url{http://ilk.uvt.nl/conll/} (zuletzt abgerufen am 11.04.16)} vor,
  wobei jedem Wort und dessen Annotationen eine ganze Zeile gewidmet ist.
  Stazgrenzen werden durch XML-Tags getrennt. Summa summarum enthält das Korpus 624.767.747 Sätze mit 11.660.894.000 Tokens.\\

  Für diese Arbeit wurden auf Basis der Ressource zwei Versionen für das Training der Wortvektoren erstellt:
  \begin{itemize}
      \item Eine Datei mit den originalen Tokens durch Leerzeichen getrennt, je ein Satz pro Zeile.
      \item Eine Datei mit den lemmatisierten Tokens durch Leerzeichen getrennt, je ein Satz pro Zeile.
  \end{itemize}

  Um die beiden Varianten zu erstellen, wurden die jeweils relevanten Informationen aus den dazugehörigen Spalten des
  \textsc{CoNLL}-Formats verwendet.\\
  Darüber hinaus wurden für spätere Anwendungen in Kapitel \ref{Chapter7} alle Eigennamen sowie die Satz-IDs für
  alle Eigennamen gesammelt (siehe die Kookkurrenzeinschränkung \ref{form:lambda-cooc}).

  \subsection{Training der Wortvektoren}\label{sec:vectrain}

  Wortvektoren wurden mithilfe des Tools \verb|word2vec| und zwei verschiedenen Modellen trainiert: \emph{Continuous Bag of Words} (CBOW)
  und Skip-Gram. Das CBOW-Model wurde zuerst von (\cite{mikolov2013efficient}) vorgestellt. Die Erklärung der Funktionsweise
  wird im nachfolgenden Teil recht klein gehalten, für eine ausführlichere und verständliche Ausführung wird beispielsweise
  die Arbeit von (\cite{rong2014word2vec}) oder die Ausführung in Kapitel \ref{sec:wordvec} empfohlen.\\

  Als Eingabe wird eine Textressource benötigt, die einen Satz pro Zeile enthält, wobei Tokens durch Leerzeichen getrennt sind;
  die Wortvektoren können entweder in einem einfachen Text- oder Binärformat gespeichert werden.\\
  Das Tool lässt zudem dem Nutzer offen, einige Parameter zu verändern. Jene, die in dieser Arbeit berücksichtigt wurden, sollen
  dabei näher erläutert werden:
  \begin{itemize}
    \item \verb|-sample|\\Die Wahrscheinlichkeit, mit der der Einfluss hochfrequenter Worte im Training geregelt wird
    \item \verb|-cbow|\\Bestimmt, welche Trainingsmethode verwendet wird ($0\ \hat{=}$ Skip-gram, $1\ \hat{=}$ Continuous-Bag-of-Words)
    \item \verb|-negative|\\Anzahl von negativen Beispielen beim Training.
  \end{itemize}

  Zwar bietet das Tool auch noch andere Parameter, jedoch soll aufgrund der Empfehlungen in (\citeauthor{levy2015improving}), in
  der eine große Anzahl von Konfigurationen ausprobiert wurde, im Rahmen dieser Arbeit nur mit den oben genannten Werten experimentiert werden.
  Weiterführende Optimierungen wurden zudem unterlassen, um den Rahmen dieser Arbeit nicht zu sprengen.\\

  \begin{figure}[h]
    \centering
    \begin{tabular}{L|L|L|L|L|L}
        \rowfont{\huge}%
        01 & DE & R & S & 5 & -5 \\
        \rowfont{\scriptsize}%
        ID & \parbox[t]{1cm}{Korpus-kürzel} & \parbox[t]{1cm}{Lemmata (L) / Roh (R)} & \parbox[t]{1.5cm}{Skip-Gram (S) / CBOW (C)} & \parbox[t]{1cm}{Anzahl Negativ-beispiele} & \parbox[t]{1.5cm}{Sampling-rate} \\
    \end{tabular}
  \caption[Erklärung der Nomenklatur für Datensets aus Wortkontextvektoren]{Erklärung der Nomenklatur für Datensets aus
  Wortkontextvektoren, hier für 01DERS5-5.\label{fig:nomenklatur}}
  \end{figure}

  Im Folgenden wird für ein spezielle Nomenklatur für jedes Vektordatenset eingeführt (siehe Abbildung \ref{fig:nomenklatur}):
  So bezeichnen die ersten beiden
  Ziffern die Nummer des jeweiligen Datensets, die nächsten beiden Buchstaben das Korpus, das für das Training verwendet
  wurde, das nächste Zeichen, ob das Korpus lemmatisiert (L) oder roh vorlag (R). Danach folgt ein Hinweis auf das im Training
  verwendete Modell (also Skip-Gram (S) oder CBOW (C)) und schlussendlich die Anzahl der negativen Trainingsbeispielen
  sowie die Samplingrate.


\section{Evaluationsarten für Wortkontextvektoren}\label{sec:we-eval}

An dieser Stelle sollen die verschiedenen Ansätze zum Evaluieren von Wortkontextvektoren miteinander verglichen werden. Zu diesem Zweck sollte zuerst eine Frage gestellt werden:
Was macht eine Menge von Wortkontextvektoren ``besser'' bzw. ``schlechter'' als andere?\\
Da der Vorteil von Wortkontextvektoren darin besteht, semantische Information zu beinhalten, wird diese
Frage meist dahingehend beantwortet, dass Vektoren dann als überlegen anzusehen sind, wenn sie
eine höhere semantische ``Ausdruckskraft'' besitzen. Um diese festzustellen, haben sich in Veröffentlichungen
zu diesem Thema bestimmte Vorgehensweisen durchgesetzt, die in den folgenden Abschnitten, vorgestellt, erläutert,
angewendet und kritisch reflektiert werden sollen.\\

  \subsection{Qualitative Evaluation}

  Qualitative Verfahren zur Evaluation sind meist recht simple Ansätze, die für das menschliche
  Auge leicht zu interpetierende Ergebnisse liefern. Deshalb sind sie für einen ersten Eindruck
  auch durchaus geeignet, sollten wenn möglich aber nicht als alleinige Kriterium für eine Bewertung
  verwendet werden, da sie meistens nur einen kleinen Ausschnitt aller in den Ergebnissen enthaltenen
  Informationen darstellen können. \\
  Beim Beispiel der Wortkontextvektoren werden beispielsweise einige Wörter des Vokabulars stellvertretend ausgewählt
  und zu diesen die \emph{k} nächsten Nachbarn\footnote{I.d.R. basierend auf der euklidischen Distanz.} im Vektorraum gesucht.
  Unter der Annahme, dass in Vektorräumen von Wortkontextvektoren ähnliche Wörter nahe beieinanderliegen, sollte diese Liste
  im Idealfall eng verwandte Begriffe zutage fördern (siehe Abbildung \ref{fig:wortliste}).\\

  \begin{table}[h]
    \centering
    \begin{changemargin}{0cm}{0cm}
    \def\arraystretch{1.5}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}lllll@{}}
      \toprule[1.5pt]
      \textsc{Name} & \textsc{Frankreich} & \textsc{Bank} & \textsc{Computerlinguistik} & \textsc{gekratzt} \\
      \toprule
      \textbf{01DERS5-5} & \textsc{Belgien} & \textsc{Anlagebank} & \textsc{Linguistik} & \textsc{gekrazt} \\
       & \textsc{Italien} & \textsc{(unknown)\_Bank} & \textsc{Informationswissenschaft} & \textsc{weggekrazt} \\
       & \textsc{Niederland} & \textsc{BAnk} & \textsc{Texttechnologie} & \textsc{geschubbert} \\
       & \textsc{Niederlande} & \textsc{Bankstatus} & \textsc{Sprachwissenschaft} & \textsc{gekratz} \\
       & \textsc{Spanien} & \textsc{Geldinstitut} & \textsc{Softwaretechnik} & \textsc{abgeleckt} \\
      \midrule
      \textbf{08DERC5-4} & \textsc{Italien} & \textsc{Hausbank} & \textsc{Bioinformatik} & \textsc{gekrazt} \\
      & \textsc{Ungarn} & \textsc{Geschäftsbank} & \textsc{Texttechnologie} & \textsc{weggekrazt} \\
      & \textsc{Polen} & \textsc{Großbank} & \textsc{Informationswissenschaft} & \textsc{geschubbert} \\
      & \textsc{Spanien} & \textsc{Kreditbank} & \textsc{Softwaretechnik} & \textsc{angeknabbert} \\
      & \textsc{Belgien} & \textsc{Mutterbank} & \textsc{Linguistik} & \textsc{rausgerissen} \\
      \midrule
      \textbf{22DELC5-2} & \textsc{Italien} & \textsc{Kreditinstitut} & \textsc{Bioinformatik} & \textsc{geritzt} \\
      & \textsc{Ungarn} & \textsc{Geldinstitut} & \textsc{Wirtschaftsgeographie} & \textsc{angesengt} \\
      & \textsc{Polen} & \textsc{Sparkasse} & \textsc{Grundschulpädagogik} & \textsc{zusammengenäht} \\
      & \textsc{Spanien} & \textsc{Landesbank} & \textsc{Computerwissenschaft} & \textsc{aufgesprungen} \\
      & \textsc{Belgien} & \textsc{Finanzinstitut} & \textsc{Humangeographie} & \textsc{tupfen} \\
      \bottomrule[1.25pt]
    \end{tabular}%
    }
  \end{changemargin}
    \caption[Listen der \emph{k} nächsten Nachbarn von Wörtern in verschiedenen Datensets]{Listen der \emph{k} nächsten Nachbarn von Wörtern in verschiedenen Datensets. Inspiriert
            von (\cite{collobert2011natural}). \label{fig:wortliste}}
  \end{table}

  Das Problem bei dieser Methode liegt in der menschlichen Subjektivität: Die präsentierte Auswahl der Begriffe
  muss nicht zwangsläufig repräsentativ für die restlichen Daten sein und könnte theoretisch aus den wenigen,
  gut funktionierenden Beispielen bestehen. Darüber hinaus bleibt es in einigen Fällen schwierig,
  die Ergebisse verschiedener Datensets zu vergleichen, da sich die Qualität der \emph{k} Nachbarn
  nicht quantifizieren lässt: Es lässt sich vielleicht erkennen, das diese in einem Fall wenig Sinn machen und
  im anderen Fall die Erwartungen erfüllen; an anderer Stelle scheinen die Resultate für den Betrachter jedoch nicht
  unbedingt schlechter, sondern einfach nur anders.\\
  Darum ist wiederum festzuhalten, dass sich qualitative Methoden in diesem Fall eher für den ersten Eindruck
  eignen, weiterhin aber Prozeduren mit quantifizierbaren Ergebnissen verwendet werden sollten, wie z.B.
  nächsten Abschnitt \ref{sec:quanteval} beschrieben werden.

  \subsection{Quantitative Evaluation}\label{sec:quanteval}

  Bei der quantitativen Evaluation von Wortkontextvektoren werden die folgenden Ideen aufgegriffen:\\
  \begin{enumerate}
    \item \textbf{Benchmark-Tests}\\
      Bei dieser pragmatischen Art der Bewertung werden wird das Datenset als Grundlage für
      einfach Aufgaben wie Sentiment-Klassifikation oder Part-of-Speech-Tagging verwendet.
      Die Qualität der Daten wird dann anhand der Ergebnisse des Systems gemessen.\\
      Diese extrinische Evaluationsmethode macht ergo nur dann Sinn, wenn man mehr als ein Datenset
      miteinander vergleicht. Dabei muss sichergestellt werden, dass die Tests immer unter den
      selben Bedingungen ablaufen, damit eine Vergleichbarkeit gewährleistet bleibt.\\
    \item \textbf{Wortähnlichkeit}\\
      Hierbei werden Wortpaaren Ähnlichkeitswerte von menschlichen Annotatoren zugeordnet. Anschließend
      werden mit den zu Verfügung stehenden Vektoren Ähnlichkeitswerte für die gleichen Paare berechnet,
      in der Regel mithilfe der Kosinus-Ähnlichkeit. Diese beschreibt die Ähnlichkeit zweier Vektoren
      als den Winkel zwischen ihnen, mit einem Wert von $-1$ ($\hat{=} 180^{\circ}$ bzw. komplett unterschiedlich) über 0
      ($\hat{=} 90^\circ$) und $+1$ ($\hat{=} 0^\circ$ bzw. Äquivalenz):
      \begin{equation}\label{form:cossim}
        cos(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}
      \end{equation}
      Danach kann mit \emph{Spearman's $\rho$}
      \footnote{Auch \emph{Spearman's rank correlation coefficient}.
      Berechnet wird dieser für zwei Mengen von Datenpunkten $X=\{x_i\}^n_{i=1}$ und $Y=\{y_i\}^n_{i=1}$ durch
      \[
      r_s = \frac{\sum_i(rg(x_i)-\overline{rg}_x)(rg(y_i)-\overline{rg}_y)}{\sqrt{\sum_i(rg(x_i)-\overline{rg}_x)^2} \sqrt{\sum_i(rg(y_i)-\overline{rg}_y)^2}}
      \]
      wobei jedem Wert $x_i$ und $x_j$ ein Rang $rg(\cdot)$ zugewiesen wird. $\overline{rg}_{(\cdot)}$ entspricht dem durchschnittlichen
      Rang.
      }.
      anschließend festgestellt werden, ob die beiden Werte für die Wortpaare korrelieren, sprich ob
      das System Paaren, denen von Menschen ein hoher Ähnlichkeitswert zugewiesen wurde, auch eine hohe
      Ähnlichkeit zuschreibt. Dabei zeigt $\rho \in [-1, 1]$ den Grad der Korrelation, wobei
      $-1$ einer starken negativen, $+1$ einer starken positiven Korrelation entspricht.
    \item \textbf{Analogien}\\
      Die dritte Methode basiert auf Analogien der Form \emph{a verhält sich zu a' wie b zu b'}.
      Die Daten werden nun dahingehend getestet, indem unter Gebrauch der Cosinus-Ähnlichkeit
      das $\tilde{b}$ aus dem Vokabular $\mathcal{V}$ gesucht wird, welches besonders ähnlich zu $b$ und $a'$ aber unähnlich zu $a$ ist:
      \begin{equation}
        b' = \underset{\tilde{b}\ \in\ \mathcal{V}}{argmax}\ cos(\tilde{b}, b - a + a')
      \end{equation}
      Sind alle Vektoren der Länge eins, so kann diese Gleichung aufgrund der obigen Definition der Kosinusähnlichkeit
      (\ref{form:cossim}) umformuliert werden:
      \begin{equation}
        b' = \underset{\tilde{b}\ \in\ \mathcal{V}}{argmax}\ cos(\tilde{b}, b) - cos(\tilde{b}, a) + cos(\tilde{b}, a')
      \end{equation}
      Diese Methode wird gemeinhin als \textsc{3CosAdd} bezeichnet. (\cite{levy2014linguistic}) etablierten dazu jedoch
      eine Alternative namens \textsc{3CosMul}\footnote{Um nur Werte im Bereich $[0,1]$ zu erhalten, wird die Formel
      für die Kosinusähnlichkeit folgendermaßen angepasst:
      \[
        cos'(\vec{a}, \vec{b}) = \frac{cos(\vec{a}, \vec{b}) + 1}{2}
      \]}, die bei Tests bessere Ergebnisse produziert:
      \begin{equation}
        b' = \underset{\tilde{b} \in \mathcal{V}}{argmax} \frac{cos'(\tilde{b}, b)\ cos'(\tilde{b}, a')}{cos'(\tilde{b}, a) + \epsilon}
      \end{equation}
      Dabei ist $\epsilon = 0,001$, um die Divison durch Null zu verhindern.\\
      Der Erfolg der Evaluation kann dann als Anteil der richtig vervollständigten Analogien (bei denen $\tilde{b}^* = b^*$) gemessen werden.
  \end{enumerate}
  In dieser Arbeit sollen die Datensets durch die zweit- und drittgenannte Methode evaluiert werden.
  Ein weiterer Fallstrick liegt allerdings in der Zusammenstellung der Datensets: So liefern
  die genannten Evaluationsdatensets nur dann aussagekräftige Ergebnisse, wenn bei der Wortähnlichkeit die menschlichenen
  Annotatoren zuverlässig und sinnvoll die Paare bewertet haben (zu messen z.B. mit $Cohen's\ \kappa$) und bei
  den Analogien aus der Zusammenstellung ebendieser (soll heißen: Welche Entitäten sind enthalten? Wie oft kommen diese
  im Datenset vor? Welche semantische Relationen wurden ausgewählt? Wurden diese maschinell oder per Hand erzeugt?).\\
  Aus diesem Grund sollen die benutzten Evaluationssets im hierauf folgenden Abschnitt \ref{sec:evaldata} näher beleuchtet werden.

  \subsection{Evaluationsdaten}\label{sec:evaldata}

    \subsubsection{Wortpaarähnlichkeit}

    Im Englischen wird für die Wortähnlichkeitsevaluation häufig das \textsc{WordSim353}-Datenset von (\cite{finkelstein2001placing})
    verwendet. Dieses wurde unter dem Namen \textsc{Schm280} in deutsche portiert, wobei die Paare nicht nur einfach übersetzt, sondern die
    Ähnlichkeit auch noch von deutschen Muttersprachlern neu bewertet wurde. Es enthält insgesamt 280 Wortpaare.\footnote{Es konnte
    jedoch nicht festgestellt werden, wieviele Menschen die Wortpaare neu bewertet haben. Während des Übersetzungsschritts
    wurde eine Zahl von 12 Teilnehmern angegeben, ob jedoch genauso viele im Bewertungsschritt partizipiert haben und wie groß
    ihre Übereinstimmung war, ist nicht festzustellen.}\\ \\
    \textsc{Wortpaare65}, \textsc{Wortpaare222} und \textsc{Wortpaare350} entstammen der Arbeit von (\cite{rubenstein1965contextual}).
    Dabei werden Wortpaaren
    Werte von 0 ($\hat{=}$ vollkommen unzusammenhängend) bis 4 ($\hat{=}$ stark zusammenhängend) zugeschrieben. Die Anzahl
    der menschlichen Annotatoren sowie deren Übereinstimmung in Form von $Cohen's\ \kappa$\footnote{
    Dies ist definiert als
    \[
      \kappa = \frac{A_o - A_e}{1 - A_e}
    \]
    wobei $A_o$ die beobachtete Übereinstimmung der Annotatoren und $A_e$ die statistisch zu erwartende Übereinstimmung
    der Annotatoren bezeichnet.
    } sind in Abbildung \ref{fig:evalsets} festgehalten.

    \begin{table}[h]
      \centering
      \def\arraystretch{1.5}
      \begin{tabular}{@{}lll@{}}
        \toprule
        \textsc{Datenset} & \textsc{\#Annotatoren} & $\kappa$ \\
        \toprule
        \textsc{Schm280} & 12 (?) & ??? \\
        \textsc{Wortpaare65} & 24 & 0,81 \\
        \textsc{Wortpaare222} & 21 & 0,49 \\
        \textsc{Wortpaare350} & 8 & 0,69 \\
        \bottomrule
      \end{tabular}
      \caption[Anzahl der Annotatoren und Agreement der Wortähnlichkeitsdatensets]{Anzahl der Annotatoren und Agreement (als $Cohen's\ \kappa$) der Wortähnlichkeitsdatensets.
      \label{fig:evalsets}}
    \end{table}

    \subsubsection{Analogien}

    Die \textsc{Google semantic/syntactic analogy datasets} wurden von (\cite{mikolov2013efficient}) eingefügt und bestehen
    aus Analogien der Form \emph{a verhält sich zu a' wie b zu b'}. (\cite{koper2015multilingual}) haben diese manuell übersetzt und durch
    drei menschliche Prüfer validieren lassen. Dabei wurde die Kategorie ``adjektiv - adverb'' ausgelassen, da sie
    im Deutschen nicht (im gleichen Ausmaß wie im Englischen) existiert, wodurch 18.552 Analogien übrigbleiben. Diese werden im Folgenden einfach als
    \textsc{Google} bezeichnet.\\ \\
    \textsc{SemRel} wurde aus Synonomie-, Antonomie- und Hypernomie-Beziehungen von (\cite{koper2015multilingual}) für das Deutsche und Englische
    konstruiert. Dabei werden Substantive, Verben und Adjektive berücksichtigt. In der deutschen Variante sind 2.462
    (recht schwierige zu vervollständigende) Analogien enthalten, die aus teilweise sehr seltenen Wörter kreiert wurden
    (siehe eine Kritik dazu im nächsten Abschnitt\ref{sec:evalerg}).

  \subsection{Evaluationsergebnisse}\label{sec:evalerg}

  Im Folgenden sollen die Ergebnisse der Wortkontextvektor-Evaluation diskutiert werden. Am Ende dieses Abschnitts werden dabei
  nur die Datensets mit den besten Ergebnissen und deren Konfiguration von Parametern vorgestellt, eine ausführliche
  Übersicht findet sich aber im Appendix \ref{AppendixB}.\\

%Mark I     & \textbf{-0,8096}         & \textbf{-0,4640$_{(13)}$}   & \textbf{-0,7302$_{(13)}$}   & \textbf{-0,7094$_{(2)}$}  & \textbf{44,56}  & 1,71 \\
%Mark XIII  & \textbf{-0,8247$_{(1)}$} & \textbf{-0,5066$_{(102)}$}  & \textbf{-0,7494$_{(132)}$}  & -0,7097$_{(48)}$          & 15,51           & \textbf{3,01} \\
%Mark XIV   & -0,8106$_{(1)}$          & -0,4953$_{(102)}$           & -0,7362$_{(132)}$           & \textbf{-0,7205$_{(48)}$} & 14,51           & 2,56 \\

  \begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{%
    \def\arraystretch{1.5}
    \begin{tabular}{@{}lllllll@{}}
      \toprule
      \textsc{Name} & \textsc{Wortpaare65} & \textsc{Wortpaare222} & \textsc{Wortpaare350} & \textsc{Schm280} & \textsc{Google} & \textsc{SemRel} \\
      \toprule
      \textbf{01DERS5-5} & \textbf{-0,8096} & \textbf{-0,4640$_{(13)}$} & \textbf{-0,7302$_{(13)}$} & \textbf{-0,7094$_{(2)}$} & \textbf{44,56}  & 1,71 \\
      \textbf{13DELS5-5} & \textbf{-0,8247$_{(1)}$} & \textbf{-0,5066$_{(102)}$}  & \textbf{-0,7494$_{(132)}$} & -0,7097$_{(48)}$ & 15,51 & \textbf{3,01} \\
      \textbf{22DELC5-2} & -0,8106$_{(1)}$ & -0,4953$_{(102)}$ & -0,7362$_{(132)}$ & \textbf{-0,7205$_{(48)}$} & 14,51 & 2,56 \\
      \bottomrule
    \end{tabular}%
    }

    \caption[Evaluationsergebnisse der besten Vektordatensets]{Evaluationsergebnisse der drei besten Vektordatensets. Links:
    Bewertung durch Spearman's $\rho$. Rechts: Treffer beim Vervollständigen
    von Analogien in Prozent. Nicht gefundene Wortkontextvektoren klein in Klammern hinter dem Wert. Für eine vollständige Liste
    der Trainingsparameter jedes Datensets siehe \ref{AppendixA}. Für die gesamte Liste aller Ergebnisse siehe \ref{AppendixB}.}
  \end{table}

  Nach der Evaluation war zuallererst festzustellen, dass sowohl bei Datensets, die auf dem normalen oder lemmatisierten Korpus
  trainiert wurden, Ergebnisse mit steigender Downsamplingrate schlechter wurden. Dies war sowohl bei Training mit dem CBOW- als
  auch mit dem Skip-Gram-Modell feststellbar. Darüber hinaus lieferte Letzteres im Schnitt bessere Resultate (und benötigte
  während des Trainings auch weniger Zeit). Das Lemmatisieren führte dazu, das während der Bewertung der Sets oft einige
  Schritte nicht durchgeführt werden könnten, was anhand der in Subskript in Klammern stehenden Zahlen abzulesen ist.
  Darunter leidet leider auch wenig die Vergleichbarkeit; es erscheint aber zumindest logisch, dass bei gleichem Evaluationswert
  einem Datenset mit weniger nicht gefundenen Wortkontextvektoren eine höhere Qualität beizumessen ist. Je nach
  späterem Anwendungsgebiet kann diese Unterscheidung aber nicht relevant sein.\\
  Bei den Analogie-Datensets sind ähnliche Tendenzen sichtbar: Beim \textsc{Google}-Datenset sind die Diskrepanzen jedoch
  deutlich stärker und Vektoren des unlemmatisierten Korpus schneiden deutlich besser ab, was vor allem dadurch
  zu erklären ist, dass in diesem Set auch Flektion geprüft wird (bspw. \emph{schreiben} verhält sich zu \emph{schreibt} wie
   \emph{sagen} zu \emph{sagt}) und diese Formen durch die Lemmatisierung velorengehen. Beim \textsc{SemRel}-Datenset
  verhält sich das ganze allerdings genau umgekehrt, wenn auch die Unterschiede wesentlich feiner sind. Da hier für alle
  möglichen Formen eines Wortes nur eine Vektorrepräsentation trainiert wird, ist zu schließen, dass diese dann auch
  mehr Information in sich kodiert.\\

  Es ist ferner zu erkennen, dass einige der extern bereitgestellten Evaluationsdaten nicht sehr gut zusammengestellt wurden.
  Bei \textsc{Wortpaare222} prägen sich die Korrelationswerte nicht unter $-0,54$ aus, was dafür spricht, das die Ähnlichkeit
  der Wortpaare für sowohl für das System als möglicherweise auch für die menschlichen Annotatoren schwer einzuschätzen war
  \footnote{Beispielhaft seien hier einige Wortpaare aus \textsc{Wortpaare222} genannt, die nach Ähnlichkeit bewertet werden
  mussten, um das Problem zu illustrieren:\\
  \emph{wahrnehmen} - \emph{Grundsatzfrage} / \emph{groß} - \emph{Arbeitszeitregelung} /
  \emph{Hubschraubertyp} - \emph{einschließlich} / \emph{Büroequipment} - \emph{Institut}.}
  Beim \textsc{SemRel}-Analogienset sticht dieser Fakt noch viel drastischer heraus: Im besten Fall wurden $3\%$ (sic!)
  der Analogien richtig vom System vervollständigt. Es sei angemerkt, dass es auch dem Autor und anderen gefragten Personen
  schwerfiel, stichprobenartig ausgewählte Fragen richtig zu beantworten
  \footnote{\emph{Lieblichkeit} verhält sich zu \emph{Anmut} wie \emph{Mittelklasse} zu$\ldots$? \rotatebox[origin=c]{180}{\emph{Mittelschicht}}\\
  \emph{Zivilgesellschaft} verhält sich zu \emph{Bürgergesellschaft} wie \emph{Gegenargument} zu$\ldots$? \rotatebox[origin=c]{180}{\emph{Widerspruch}}\\
  \emph{Trio} verhält sich zu \emph{Solo}  wie \emph{Arzt} zu$\ldots$? \rotatebox[origin=c]{180}{\emph{Patient}}}.
  Deshalb stellen sich die Ergebnisse bei diesem Vergleichsset ohne klare Tendenz in den Resultaten und generell sehr schlecht dar.\\

  Für die nachfolgenden Schritte wurde die bestabschneidenen Wortkontextvektorsets bei \textsc{Google} und den anderen
  Wortpaarsets, namentlich 01DERS5-5, 13DELS5-5 und 14DELS5-4 ausgewählt.
