% Chapter 4

\chapter{Vorbereitung} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1}

%----------------------------------------------------------------------------------------

\section{Vorbereitung}
Bla

  \subsection{Extraktion von Named Entities}

  \subsection{Aufbereitung des Korpus}

  Als Textressource wurde das DECOW14X-Korpus (DE = Deutsch, COW = ``\textbf{CO}rpus from the \textbf{W}eb'') verwendet.
  Dieses Korpus von (\cite{schafer2012building})  besteht aus 21 Texten,
  die in den Jahren 2011 und 2014 von deutschsprachigen Internetseiten gecrawled und aufbereitet wurden. Dies beinhaltet
  PoS-Tagging, Chunking, Lemmatisierung, das Markieren von Eigennamen (Named Entities) und dem Hinzufügen von Metadaten.
  Die Sätze liegen darin im CoNLL-Format\footnote{Siehe http://ilk.uvt.nl/conll/ (zuletzt abgerufen am 11.04.16)} vor,
  wobei jedem Wort und dessen Annotationen eine ganze Zeile gewidmet ist,
  Stazgrenzen werden durch XML-Tags getrennt. Summa summarum enthält das Korpus 624.767.747 Sätze mit 11.660.894.000 Tokens.\\

  Für diese Arbeit wurden auf Basis der Ressource drei Version für das Training der Wortvektoren erstellt:
  \begin{itemize}
      \item Eine Datei mit den originalen Tokens durch Leerzeichen getrennt, je ein Satz pro Zeile.
      \item Eine Datei mit den lemmatisierten Tokens durch Leerzeichen getrennt, je ein Satz pro Zeile.
      \item Eine Datei mit dem lemmatisierten Tokens, sortiert nach den für jeden Satz geparsten Dependenzen, ein Satz pro Zeile.
  \end{itemize}
  Die Dependenzen wurden dabei mit dem Tool X erzeugt. [Blabla erläutern wenn Punkt erledigt.]


  \subsection{Training der Wortvektoren}

  Wortvektoren werden mithilfe des Tools \emph{word2vec} und zwei verschiedenen Modellen trainiert: Continuous-Bag-of-Words (CBOW)
  und Skip-Gram. Das CBOW-Model wurde zuerst von (\cite{mikolov2013efficient}) vorgestellt. Die Erklärung der Funktionsweise
  wird im nachfolgenden Teil recht klein gehalten, für eine ausführlichere und verständliche Ausführung wird beispielsweise
  die Arbeit von (\cite{rong2014word2vec}) empfohlen.\\

  Zum Training der Vektoren wurde das C-Tool \emph{Word2Vec} von (\citeauthor{mikolov2013efficient}) verwendet. Als Eingabe
  benötigt es eine Textressource, die einen Satz pro Zeile enthält, Tokens durch Leerzeichen getrennt und gibt die Wortvektoren
  entweder einem einfach Text- oder Binärformat aus.\\
  Das Tool lässt zudem dem Nutzer offen, einige Parameter zu verändern. Jene, die in dieser Arbeit berücksichtigt wurden, sollen
  dabei näher erläutert werden:
  \begin{itemize}
    \item \verb|-sample|\\Die Wahrscheinlichkeit, mit der hochfrequente Worte
    \item \verb|-cbow|\\Bestimmt, welche Trainingsmethode verwendet wird ($0\ \hat{=}$ Skip-gram, $1\ \hat{=}$ Continuous-Bag-of-Words)
    \item \verb|-negative|\\Anzahl von negativen Beispielen beim Training.
  \end{itemize}

  Zwar bietet das Tool auch noch andere Parameter, jedoch soll aufgrund mit der Empfehlungen in (\citeauthor{levy2015improving}), in
  der eine große Anzahl von Konfigurationen ausprobiert wurde, im Rahmen dieser Arbeit nur mit den oben genannten Werten experimentiert werden.
