% Chapter 7

\chapter{Experiment $\mathcal{B}$: Relationsvorhersage mit Wortvektorrepräsentationen} % Main chapter title

\label{Chapter7} % For referencing the chapter elsewhere, use \ref{Chapter1}

%----------------------------------------------------------------------------------------

\begin{itquote}
Q: Why did the multithreaded chicken cross the road?\\
A: to To other side . get the
\flushright
\textsc{Jason Whittington}
\end{itquote}

\section{Idee}

Gegeben ist ein Vektorraum $V$ mit Wortkontextvektoren $\vec{u}, \vec{v} \in V = {R}^d$.
Gesucht ist eine Funktion $\phi$, die ein Vektorenpaar $(\vec{u}, \vec{v})$ in einen Relationsraum abbildet:
$\phi: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^e$, wobei
nicht zwangsläufig $d = e$.\\
In ihrer einfachsten Form bildet sie einfach die Differenz $\vec{d}$ der beiden Vektoren ab:
\begin{equation}
  \phi(\vec{u}, \vec{v}) = \vec{v} - \vec{u} = \vec{d}
\end{equation}

\section{Algorithmus}

Der Grundalgorithmus (siehe Abbildung \ref{fig:algo1}) versucht nun, alle Kombinationen von Differenzen der
Wortkontextvektoren zu bilden. Die Berechnung dieser würde bei $n$ Vektoren in $O(n) = n * n = n^2$ resultieren.
Zwar ist $\phi(\vec{u}, \vec{v}) \neq \phi(\vec{v}, \vec{u})$, jedoch wäre die Berechnung beider Differenzvektoren redundant,
da sie lediglich Spiegelungen voneinander im Raum sind und so dieselbe Information enthalten. Die Berechnung
der Differenz in nur eine Richtung reduziert die Komplexität dadurch zu $O(n) = \frac{n * (n-1)}{2}$
\footnote{
Gegeben einer Menge relevanter Vektorpaarkombinationen $\mathcal{C}$ gilt also:
\[
  \forall (\vec{u}, \vec{v}) \in \mathcal{C}: (\vec{v}, \vec{u}) \notin \mathcal{C}
\]
}.

\begin{figure}[h]
  \centering
  \begin{algorithm}[H]
    \KwData{Menge von Vektorpaaren $\mathcal{C}$}
    \For{$(\vec{u}, \vec{v}) \in \mathcal{C}$}{
      $\vec{d} = \vec{v} - \vec{u}$;
    }
  \end{algorithm}
  \caption[Einfacher Projektionsalgorithmus]{Einfacher Projektionsalgorithmus.\label{fig:algo1}}
\end{figure}

Eine Modifikation des Algorithmus besteht darin, das Berechnen von $\vec{d}$ an eine Bedingung zu knüpfen, da selbst
mit der obigen Einschränkung die Ausgabe des Algorithmus gegeben einer Datenmenge immer noch sehr groß ist.\\

Gegeben sei eine Menge von Sätzen (ein Korpus) $\mathcal{K}$ mit $n$ Sätzen $s_0, \ldots, s_n$ sodass $\mathcal{K} = \{s_i\}_{i=1}^{n}$ mit
$m$ Wörtern $w_{ij}$ pro Satz $s_i = \{w_{ij}\}_{j=1}^m$. Eine Kookkurrenz von zwei Begriffen (\emph{types}) $t_1$ und $t_2$
besteht demnach, falls im Korpus mindestens ein Satz existiert, in dem beide vorkommen. Dazu können wir
eine Funktion $\Lambda(\cdot)$ definieren, die die Anzahl der Kookkurrenzen für alle Sätze $s_i$ und deren jeweiligen
Wörtern $w_{ij}$ bestimmt:
\begin{equation}\label{form:lambda-cooc}
  \Lambda(t_1, t_2) = |\{s_i | \exists w_{ij} = t_1 \land \exists w_{ij} = t_2 \land t_1\neq t_2\}_{i=1}^{n}|
\end{equation}
Eine mögliche Einschränkung besteht darin, in einem geänderten Algorithmus (siehe Abbildung \ref{fig:algo2}) das Berechnen von $\vec{d}$ nur
dann zu erlauben, wenn die Anzahl der Kookkurrenzen der zu den Vektoren $\vec{v}(t_1), \vec{v}(t_2)$ gehörenden Begriffe $t_1, t_2$
einen bestimmten Schwellenwert $\gamma$ überschreitet, also $\Lambda(t_1, t_2) \geq \gamma$.

\begin{figure}[h]
  \centering
  \begin{algorithm}[H]
    \KwData{Menge von Vektorpaaren $\mathcal{C}$}
    \For{$(\vec{v}(t_1), \vec{v}(t_2)) \in \mathcal{C}$}{
      \If{$\Lambda(t_1, t_2) > \gamma$}{
        $\vec{d} = \vec{v}(t_2) - \vec{v}(t_1)$;
      }
    }
  \end{algorithm}
  \caption[Modifizierter Projektionsalgorithmus]{Modifizierter Projektionsalgorithmus, bei dem die zu den Vektoren gehörigen
  Begriffe über $\gamma$ Mal im Korpus im gleichen Satz aufgetreten sein müssen, damit $\vec{d}$ errechnet wird.\label{fig:algo2}}
\end{figure}


\section{Parallelisierter Algorithmus}\label{sec:para-algo}

Doch selbst mit den erwähnten Einschränkungen skaliert dieser Algorithmus nur bedingt. Um dem entgegenzuwirken, soll nun
versucht werden, diesen mithilfe von \emph{Multithreading} zu parallelisieren. Bei dieser Technik beschäftigt ein Rechenprozess
mehrere Prozessstränge (\emph{Threads}), die miteinander kommunizieren und bei Bedarf synchronisiert werden können,
während sie Teile eines Problems gleichzeitig lösen.\\

Ein beliebtes Muster, das Verhältnis zwischen mehreren Threads zu definieren, besteht im \emph{Master-Slave}-Muster.
Dabei fungiert ein Subprozess als Aufseher, der seine Arbeiterprozesse überwacht, sie mit Daten versorgt und in manchen
Fällen untereinander koordiniert, während die anderen Prozesse Teilprobleme lösen und ggf. Rückmeldung an den Aufseherprozess
geben.\\

\begin{figure}[h]
  \centering
  \begin{algorithm}[H]
    \KwData{Menge von Vektorpaaren $\mathcal{C}$}
    \KwData{Menge von erledigten Vektorpaaren $\mathcal{C}'$ (Anfangs $\mathcal{C}' = \varnothing$)}
    \KwData{Menge von Threads $\mathcal{T}=\{th\}_i^n$}
    \BlankLine
    \textbf{Klasse} \textsc{MasterThread} \\
      data = ladeDaten();\\
      \For{$th \in \mathcal{T}$}{
        starteThread($th$, data);
      }
      \For{$th \in \mathcal{T}$}{
        beendeThread($th$);
      }

    \BlankLine
    \textbf{Klasse} \textsc{WorkerThread} \\
    \For{$(\vec{v}(t_1), \vec{v}(t_2)) \in \mathcal{C}$}{
      \If{$(\vec{v}(t_1), \vec{v}(t_2)) \notin \mathcal{C}'$}{
        \If{$\Lambda(t_1, t_2) > \gamma$}{
          $\vec{d} = \vec{v}(t_2) - \vec{v}(t_1)$;
        }
        $\mathcal{C}'$ = $\mathcal{C}' \cup (\vec{v}(t_1), \vec{v}(t_2))$;
      }
    }
  \end{algorithm}
  \caption[Parallelisierter Projektionsalgorithmus]{Parallelisierter Projektionsalgorithmus, bei dem die zu den Vektoren gehörigen
  Begriffe über $\gamma$ Mal im Korpus im gleichen Satz aufgetreten sein müssen, damit $\vec{d}$ errechnet wird. Ein Master-Thread
  verteilt zudem die Aufgaben an Worker-Threads, die diese Berechnungen übernehmen und erledigte Vektorpaare in einer Menge ablegen.
  \label{fig:algo3}}
\end{figure}

In diesem konkreten Fall ist der Master-Thread dafür verantwortlich, alle benötigten Daten in entsprechende Datenstrukturen
zu laden, sie den Worker-Threads bereitzustellen und letztere gemeinsam zu starten und zu beenden, sobald ein bestimmtes
Abschlusskriterium der Aufgabe erfüllt ist.
Zusätzlich wird eine Menge eingeführt, in der Paare von Vektoren hinzugefügt werden, sofern
ihr Differenzvektor von einem Thread ausgerechnet wurde (siehe Abbildung \ref{fig:algo3}). Damit nun keiner der Threads redundante Berechnungen durchführt, prüft er, ob sein
aktuelles Vektorpaar sich in dieser Menge befindet und schon abgehakt wurde\footnote{Damit dies möglichst schnell funktioniert,
wird das Vektorpaar durch eine Hashfunktion $h$ in einen Wert umgewandelt und in einem \emph{Hashset} gespeichert. $h(\cdot)$ wird so gewählt,
sodass $h(\vec{v}(t_1), \vec{v}(t_2)) = h(\vec{v}(t_2), \vec{v}(t_1))$.}.\\

Als Grundlage der Daten wurde das Datenset 01DERS5-5 gewählt, da es in den meisten der Evaluationsaufgaben
als bestes Abschnitt. Mit $\gamma = 1000$ resultierte das Prozedere in einer neuen Menge an Daten mit insgesamt
138.086 Differenzvektoren mit einer Größe von 117 Megabyte, ausgehend von einem Vektordatenset mit 6 Gigabyte und
ungefähr 6,6 Millionen Wortkontextvektoren.\\

\section{Evaluation}

Die Evaluation der im vorherigen Schritt gewonnen Daten findet folgendermaßen statt: Für jedes gewonnene Cluster $c$ wird für
jedes Paar von Vektoren die Zugehörigkeit der entsprechenden Wörter zu einer \textsc{Freebase}-Relation $r \in R$ geprüft.
Dem gesamten Cluster kann dann die Relation $r_c$ zugeordnet werden, die die meisten Treffer zu verzeichnet hat\footnote{Die Indikatorfuntion mit $\mathbbm{1}_{r_c}((w_1, w_2))$ liefert $1$, wenn
$(w_1, w_2) \in r$ gilt, ansonsten $0$.}:

\begin{equation}
    r_c = \underset{r \in R}{argmax}\ \sum_{(\vec{v}(w_1), \vec{v}(w_2)) \in c}  \mathbbm{1}_{r}((w_1, w_2))
\end{equation}

Die ``Reinheit'' $P$ eines Clusters kann dann errechnet werden, indem der Anteil der zur Relation des gesamten Clusters
zugehörigen Wortpaaren bestimmt wird:

\begin{equation}\label{form:purity}
    P(c) = \sum_{(\vec{v}(w_1), \vec{v}(w_2)) \in c} \frac{\mathbbm{1}_{r_c}((w_1, w_2))}{|c|}
\end{equation}

Diese Metrik bestraft es allerdings, wenn in einem Cluster neue Paare einer Relation vorkommen, die noch nicht in \textsc{Freebase}
vorhanden sind. Da es allerdings auch ein Teilziel dieses Ansatzes ist, ebensolche Paare neu aufzufinden, wird noch
eine modifizierte Version dieser Metrik behandelt: Es gilt dabei vor allem, unbekannte richtige Wortpaare von falschen
zu unterscheiden. Zu diesem Zweck wird die Annahme getroffen, dass alle Vektoren $\vec{v}(h)$ und $\vec{v}(t)$, die jeweils zu den Kopfentitäten $H_c$ und
den Fußentitäten $T_c$\footnote{$H_c = \{\vec{v}(w_1)|(w_1, \cdot) \in r_c\}$ und $T_c = \{\vec{v}(w_2)|(\cdot, w_2) \in r_c\}$} gehören,
untereinander ähnlich sind\footnote{In einem Cluster der Relation \emph{Hauptstadt von} würden
sich vermutlich die Vektoren aller Hauptstädte wie \emph{Paris}, \emph{Berlin},$\ldots$ ebenso ähneln wie die der dazugehörigen
Länder \emph{Frankreich}, \emph{Deutschland},$\ldots$}. So wird für jedes Cluster zunächst für ebendiese Entitäten ein Durchschnitt
errechnet, deren Zugehörigkeit zu $r_c$ festgestellt wurde:

\begin{equation}
  \overline{h} = \sum_{\vec{v}(w_1) \in H_c}\frac{\vec{v}(w_1)}{|H_c|}
\end{equation}

\begin{equation}
  \overline{t} = \sum_{\vec{v}(w_2) \in H_c}\frac{\vec{v}(w_2)}{|T_c|}
\end{equation}

Danach erweitern wir $r_c$ um die Wortpaare, deren Vektoren den durchschnittlichen Kopf- und Fußvektoren ähnlich genug sind.
Zu diesem Zweck wird ein Schwellenwert $\tau$ definiert, den ein potenziell zu $r_c$ zugehöriges Wortpaar im Bezug auf
Cosinusänlichkeit (siehe \ref{form:cossim}) nicht unterschreiten darf:

\begin{equation}
  r_c^+ = r_c\ \cup\ \{(w_1, w_2)| (\vec{v}(w_1), \vec{v}(w_2)) \in c\ \land\ cos(\vec{v}(w_1), \overline{h}) \geq \tau\ \land\ cos(\vec{v}(w_2), \overline{t}) \geq \tau\}
\end{equation}

Auf Basis der Gleichung \ref{form:purity} folgt für die modifizierte Reinheit eines Wortpaarclusters $P^+$:

\begin{equation}
  P^+(c) = \sum_{(\vec{v}(w_1), \vec{v}(w_2)) \in c} \frac{\mathbbm{1}_{r_c^+}((w_1, w_2))}{|c|}
\end{equation}

\section{Ergebnisse}

Vorgreifend soll angemerkt werden, dass es nicht möglich war, endgültige Ergebnisse mithilfe der vorher beschriebenen
Evaluationsmetrik $P^+(\cdot)$ zu erzeugen. Die genauen Gründe hierfür werden im nächsten Abschnitt \ref{sec:zwi-dis}
genauer beleuchtet. Dementsprechend sollen an dieser Stelle lediglich Zwischenergebnisse vorgestellt werden.\\

\begin{figure}[h]
  \centering
  \begin{multicols}{2}
    \textbf{Cluster A}\\
    \textsc{Oldenburg} - \textsc{Görlitz}\\
    \textsc{Tschechien} - \textsc{Karin}\\
    \textsc{Tschechien} - \textsc{Uwe}\\
    \textsc{Hockenheim} - \textsc{Offenburg}\\
    \textsc{Oldenburg} - \textsc{Offenburg}\\
    $\vdots$\\
    \columnbreak
    \textbf{Cluster B}\\
    \textsc{Pfeiffer} - \textsc{EU-Länder}\\
    \textsc{Franz} - \textsc{EU-Länder}\\
    \textsc{Uwe} - \textsc{EU-Länder}\\
    \textsc{Karin} - \textsc{EU-Länder}\\
    \textsc{Henning} - \textsc{EU-Länder}\\
    $\vdots$\\
  \end{multicols}
  \begin{multicols}{2}
    \textbf{Cluster C}\\
    \textsc{Nordhausen} - \textsc{Oberliga}\\
    \textsc{Bochum} - \textsc{Regionalliga}\\
    \textsc{Hameln} - \textsc{Regionalliga}\\
    \textsc{Chemnitzer} - \textsc{Regionalliga}\\
    \textsc{Chemnitzer} - \textsc{Bundesliga}\\
    $\vdots$\\
    \columnbreak
    \textbf{Cluster D}\\
    \textsc{Dieter} - \textsc{italienischen}\\
    \textsc{Pirna} - \textsc{italienischen}\\
    \textsc{Damon} - \textsc{italienischen}\\
    \textsc{Shanghai} - \textsc{italienischen}\\
    \textsc{Pankow} - \textsc{italienischen}\\
    $\vdots$\\
  \end{multicols}
  \caption[Auszug aus verschiedenen Wortpaarclustern]{Auszüge aus einigen Wortpaarclustern, die mit in dem Abschnitt
  \ref{sec:para-algo} vorgestellten Ansatz erzeugt wurden.\label{fig:clusters}}
\end{figure}

Wie Abbildung \ref{fig:clusters} zu entnehmen ist, in der einige Auszüge aus Clustern beispielhaft dargestellt sind,
fällt es schwer, die dort aufgeführten Wortpaare insgesamt einer Relation zuzuordnen. Im Gegenteil gelingt dies selbst für
einzelne Paare nicht, siehe beispielweise ``Uwe'' und ``EU-Länder''.\\

Es zeigte sich, dass auch das Kookkurrenz-Feature zu keiner Besserung der Ergebnisse geführt hat. Aufgrund der scheinbaren
Zufälligkeit der Ergebnisse erschien es auch nicht mehr sinnvoll, weitere Schritte (vor allem die Evaluation) auf Basis dieser Daten
durchzuführen.\\

\section{Zwischenfazit}\label{sec:zwi-dis}

Wie im letzten Abschnitt zu erkennen ist, konnte keine signifikante Verbesserung der Ergebnisse erzielt werden.
Im Folgenden soll ein Versuch unternommen werden zu erklären, welche Probleme zu diesen Resultaten geführt haben und welche
Implikationen diese für generelle Bestrebungen auf diesem Gebiet besitzen.

\subsection{Daten}\label{sec:zwi-dis-data}

Will man das Scheitern unmittelbar auf einen Faktor zurückführen, so liegen die verwendeten Daten am nächsten.
Es lassen sich im Bezug auf jene folgende Punkte feststellen:
\begin{itemize}
  \item \textbf{Menge}\\ Selbst mit der Reduzierung der resultierenden Daten von $n^2$ auf $\frac{n*(n-1)}{2}$ und weiter
  mit $\Lambda(\cdot)$ ist die
  Datenmenge noch sehr groß, gerade wenn das anfängliche Wortkontextvektorset auf einem großen Vokabular wie dem des \textsc{Decow}-Korpus aufbaut.
  Dies stellt hohe Anforderung an die Skalierbarkeit aller involvierter Algorithmen und fordert Einschränkungen auf verschiedenen
  Ebenen, wodurch mögliche spätere Entdeckungen eventuell vorenthalten oder verzerrt werden könnten.
  \item \textbf{Rauschen}\\ In den Enddaten ist der überwiegende Teil der Datenpunkte keiner sinnvollen Relation zuzuordnen.
  Dadurch verrauschen mögliche sinnvolle Relationscluster, die darin untergehen (vgl. Abbildung \ref{fig:proj_map}). Die resultierenden Cluster sind sehr
  diffus, da sie sich schlecht vom Hintergrundrauschen abgrenzen. Dies ist beispielsweise durch den Wert des Silhouettenkoeffizienten\footnote{
  Der Silhoettekoeffizient $s_C$ beschreibt das durchschnittliche Verhältnis vom Abstand eines Punktes zu seinem Clusterzentrum gegnüber
  des nächsten Clusterzentrums. $-1 \leq s_C \leq 1$.}
  erkennbar, der bei den Experimenten immer kurz unter Null lag\footnote{Für ein gutes Clustering wird ein Wert von $s_C \geq 0,75$
  erwartet.}. Dass in diesem Rauschen valide Ergebnisse vorliegen müssten, lassen das Clustering vorgefilterter Wortpaare von (\cite{fu2014learning}) und Abbildung \ref{fig:capitals}
  zwar erahnen, sie waren jedoch nicht auszumachen.
  \item \textbf{Ähnliche Distanzvektoren}\\
  Der beschriebene Ansatz wurde unter der Annahme verfolgt, dass für Wortpaare einer Relation $r=\{(h_j, t_j)\}_{j=0}^m$
  folgendes gilt:
  \begin{equation}
    \vec{v}(t_0) - \vec{v}(h_0) \approx \vec{v}(t_1) - \vec{v}(h_1) \approx \ldots \approx \vec{v}(t_m) - \vec{v}(h_m)
  \end{equation}
  Es wurde aufgrund vorheriger Arbeiten die Hypothese aufgestellt, dass sich die Distanzvektoren von
  Wortpaaren derselben Relation ähneln. Durch die Ergebnisse wurde diese Hypothese nicht widerlegt (im Gegenteil scheinen
  andere Arbeiten wie (\cite{bordes2013translating}) oder (\cite{lin2015learning}) diese Annahmen zu bestätigen), jedoch
  gilt diese Eigenschaft nicht \textbf{exklusiv} für diese Untermenge an Vektorenpaaren.\\

  Nehmen wir beispielsweise im ursprünglichen Vektorraum mit Wortvektoren das folgende Szenario an: Wir finden dort
  zwei Cluster $c_1$ und $c_2$ vor. Die zu den Vektoren zugehörigen Cluster weisen eine semantische Ähnlichkeit auf
  und liegen deshalb nahe beieinander, z.B. Vornamen wie \emph{Julia} und \emph{Hans} in $c_1$ und Städte wie
  \emph{Barcelona} und \emph{Paris} in $c_2$. Daraus ergibt sich Folgendes:
  \begin{equation}
    \begin{split}
      \vec{v}(Julia) \approx \vec{v}(Hans) \land \vec{v}(Barcelona) \approx \vec{v}(Paris) \implies \\
      \vec{v}(Julia) - \vec{v}(Barcelona) \approx \vec{v}(Hans) - \vec{v}(Paris)
    \end{split}
  \end{equation}
  Anders ausgedrückt: Auch wenn sich semantische Information in den Wortvektoren dadurch manifestiert, dass ähnliche
  Ausdrücke in ihren Clustern nahe beinander liegen, können Distanzvektoren aus Wortenpaaren der gleichen Cluster ähnlich sein,
  ohne semantisch in irgendeinem Zusammenhang zu stehen. Dies führt schlussendlich dazu, dass die wenigen Cluster, die
  im Relationsraum gefunden werden, zwar aus Wortpaaren mit ähnlichem Differenzvektor bestehen, jedoch keine ``sinnvolle''
  Relation bilden. Dieses Phänomen lässt sich mit dem in \ref{form:lambda-cooc} eingeführten Parameter $\gamma$ zwar verringern, allerdings nicht gänzlich
  verhindern lässt. Das Ziel, neue und legitime Relationen bzw. Wortpaare zu finden, wird durch diesen Trugschluss ad absurdum geführt.

\begin{figure}[h]
  \centering
  \begin{multicols}{2}
    \includegraphics[width=0.5\textwidth]{../img/mappings_get10000_occ100_cos.jpg}
    \textbf{Mappings A:} $\spadesuit\clubsuit\blacksquare$
    \includegraphics[width=0.5\textwidth]{../img/mappings_get10000_occ100_eucl.jpg}
    \textbf{Mappings C:} $\spadesuit\clubsuit\varheart$

    \columnbreak

    \includegraphics[width=0.5\textwidth]{../img/mappings_get10000_occ100.jpg}
    \textbf{Mappings B:} $\spadesuit\clubsuit$
    \includegraphics[width=0.5\textwidth]{../img/mappings10000_occ.jpg}
    \textbf{Mappings D:} $\spadesuit\bigstar$
  \end{multicols}

  \flushleft
  \fbox{
  \parbox{\textwidth}{
  \textbf{Feature-Legende}
  \begin{multicols}{2}
  \begin{itemize}
    \item[$\spadesuit$] $\vec{v}(t_2) - \vec{v}(t_1)$
    \item[$\bigstar$] $\Lambda(t_1, t_2) \geq 1$
    \item[$\clubsuit$] $\Lambda(t_1, t_2) \geq 100$
  \end{itemize}
  \columnbreak
  \begin{itemize}
    \item[$\blacksquare$] $cos(\vec{v}(t_2), \vec{v}(t_1))$
    \item[$\varheart$] $\parallel \vec{v}(t_2) - \vec{v}(t_1) \parallel$
  \end{itemize}
\end{multicols}
}}
  \caption[Dreidimensionale Projektionen einiger durch das Mappingverfahren resultierender Vektorräume]{Dreidimensionale
   Projektionen einiger zufällig ausgewählter, durch das Mappingverfahren resultierender Vektorräume.
   Jeweils dargestellt: 10000 Vektoren. Symbole zeigen die jeweils genutzten Features an, die zu den resultierenden
   Vektoren konkateniert oder im Falle von $\Lambda(\cdot)$ im Vornherein als Filter verwendet werden.\label{fig:proj_map}}
\end{figure}

\end{itemize}

\subsection{Ansatz}

Das Fehlschlagen des Vorgehens kann auch auf einer theoretischen Ebene festgestellt werden: Das Ziel bestand darin, Wissen
über Relationen zwischen Wortpaaren zu extrahieren. Um dieses Wissen gewissermaßen ``freizulegen'', muss es aber auf eine
Art und Weise innerhalb der Daten ``kodiert'' sein, wenn auch implizit (so wie die semantische Ähnlichkeit zwischen Wörtern
durch die räumliche Nähe ihrer Vektoren und deren Dimension kodiert ist).\\
Wie beim Trugschluss am Ende von \ref{sec:zwi-dis-data} gezeigt wurde, sind semantische Relationen nicht eindeutig
innerhalb der Daten festzustelen - bzw. nur dann, wenn bereits vorher bruchstückhaftes Wissen darüber vorliegt. Ohne
dieses Vorwissen sind richtige von nur scheinbaren Relationen nicht zu trennen. Um den Ansatz zukünftig zu einen erfolgreichen
Ende zu führen, müsste also überlegt werden, ihn von einem unüberwachten in einen mindestens semi-überwachten Ansatz zu
transformieren und weiteres Wissen aus einer anderen Domäne hinzuzufügen.
