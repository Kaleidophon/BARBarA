% Chapter 5

\chapter{Evaluation der Wortvektoren} % Main chapter title

\label{Chapter5} % For referencing the chapter elsewhere, use \ref{Chapter1}

%----------------------------------------------------------------------------------------

\begin{itquote}
[Y]ou’ll see that there are a lot of pairings where words with similar meanings are nearby. [...]
On the other hand, there’s a lot of junk. [...] You’d want the closest word to ``grandma'' to be ``grandpa'', not ``gym.''
\flushright
\textsc{Ben Schmidt über das Plotten von Word Embeddings in zwei Dimensionen\footnote{Blogeintrag ``Word Embeddings for the digital humanities'' von Ben Schmidt (2015),
online unter http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html (zuletzt abgerufen am 21.04.15)}}
\end{itquote}


\section{Evaluation der Wortvektoren}

An dieser Stelle sollen die verschiedenen Ansätze zum Trainieren von Wortvektoren, die im vorherigen Kapitel
vorgestellt werden, miteinander verglichen werden. Zu diesem Zweck sollte zuerst eine Frage gestellt werden:
Was macht eine Menge von Wortvektoren ``besser'' bzw. ``schlechter'' als andere?\\
Da der Vorteil von Wortvektoren darin besteht, semantische Informationen zu beinhalten, wird diese
Frage meist dahingehend beantwortet, dass Vektoren dann als überlegen an zu sehen sind, wenn sie
eine höhere semantische Ausdruckskraft besitzen. Um dies festzustellen, haben sich in Veröffentlichungen
zu diesem Thema bestimmte Vorgehensweisen durchgesetzt, die in dne folgenden Abschnitten, vorgestellt, erläutert,
angewendet und kritisch reflektiert werden sollen.\\

  \subsection{Qualitative Evaluation}

  Qualitative Verfahren zur Evaluation sind meist recht simple Ansätze, die für das menschliche
  Auge leicht zu interpetierbare Ergebnisse liefern. Deshalb sind sie für einen ersten Ausdruck
  auch durchaus geeignet, sollten wenn möglich aber nicht als alleinige Kriterium für eine Bewertung
  hinzugezogen werden, da sie meisten nie die Gesamtheit aller in den Ergebnissen enthaltenen
  Informationen darstellen können. \\
  Im Beispiel der Wortvektoren werden beispielsweise einige Wörter des Vokabulars stellvertretend ausgewählt
  und zu diesen die \emph{k} nächsten Nachbarn im Vektorraum gesucht. Unter der Annahme, dass
  in Vektorräumen von Wortvektoren ähnliche Wörter nahe zusammenliegen, sollte diese Liste nah verwandte
  Begriffe zutage fördern (siehe Abbildung \ref{fig:wortliste}).\\

  \begin{figure}[h]
    \centering
    \begin{changemargin}{0cm}{0cm}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|cccc}
       & \textsc{Frankreich} & \textsc{Bank} & \textsc{Computerlinguistik} & \textsc{gekratzt} \\
      \hline \hline
      \multirow{5}{*}{Mark I} & \textsc{Belgien} & \textsc{Anlagebank} & \textsc{Linguistik} & \textsc{gekrazt} \\
       & \textsc{Italien} & \textsc{(unknown)\_Bank} & \textsc{Informationswissenschaft} & \textsc{weggekrazt} \\
       & \textsc{Niederland} & \textsc{BAnk} & \textsc{Texttechnologie} & \textsc{geschubbert} \\
       & \textsc{Niederlande} & \textsc{Bankstatus} & \textsc{Sprachwissenschaft} & \textsc{gekratz} \\
       & \textsc{Spanien} & \textsc{Geldinstitut} & \textsc{Softwaretechnik} & \textsc{abgeleckt} \\
      \hline
      \multirow{5}{*}{Mark VIII} & \textsc{Italien} & \textsc{Hausbank} & \textsc{Bioinformatik} & \textsc{gekrazt} \\
      & \textsc{Ungarn} & \textsc{Geschäftsbank} & \textsc{Texttechnologie} & \textsc{weggekrazt} \\
      & \textsc{Polen} & \textsc{Großbank} & \textsc{Informationswissenschaft} & \textsc{geschubbert} \\
      & \textsc{Spanien} & \textsc{Kreditbank} & \textsc{Softwaretechnik} & \textsc{angeknabbert} \\
      & \textsc{Belgien} & \textsc{Mutterbank} & \textsc{Linguistik} & \textsc{rausgerissen} \\
      \hline
      \multirow{5}{*}{Mark XXII} & \textsc{Italien} & \textsc{Kreditinstitut} & \textsc{Bioinformatik} & \textsc{geritzt} \\
      & \textsc{Ungarn} & \textsc{Geldinstitut} & \textsc{Wirtschaftsgeographie} & \textsc{angesengt} \\
      & \textsc{Polen} & \textsc{Sparkasse} & \textsc{Grundschulpädagogik} & \textsc{zusammengenäht} \\
      & \textsc{Spanien} & \textsc{Landesbank} & \textsc{Computerwissenschaft} & \textsc{aufgesprungen} \\
      & \textsc{Belgien} & \textsc{Finanzinstitut} & \textsc{Humangeographie} & \textsc{tupfen} \\
    \end{tabular}%
    }
  \end{changemargin}
    \caption{Listen der \emph{k} nächsten Nachbarn von Wörtern in verschiedenen Datensets. Inspiriert
            von (\cite{collobert2011natural}). \label{fig:wortliste}}
  \end{figure}

  Das Problem bei dieser Methode liegt in der menschlichen Subjektivität: Die präsentierte Auswahl der Begriffe
  muss nicht zwangsläufig repräsentativ für die restlichen Daten sein und könnte theoretisch aus den wenigen,
  gut funktionierenden Beispielen bestehen. Darüber hinaus bleibt es in einigen Fällen schwierig,
  die Ergebisse verschiedener Datensets zu vergleichen, da sich die Qualität der \emph{k} Nachbarn
  nicht quantifizieren lässt: Es lässt sich vielleicht erkennen, das diese in einem Fall wenig Sinn machen und
  im anderen Fall die Erwartungen erfüllen; an anderer Stelle scheinen die Resultate für den Betrachter jedoch nicht
  unbedingt schlechter, sondern einfach nur anders.\\
  Darum ist wiederum festzuhalten, dass sich qualitative Methoden in diesem Fall eher für den ersten Eindruck
  eignen, weiterhin aber Prozeduren mit quantifizierbaren Ergebnissen verwendet werden sollten, wie z.B.
  nächsten Abschnitt \ref{sec:quanteval} beschrieben werden.

  \subsection{Quantitative Evaluation}\label{sec:quanteval}

  Bei der quantitativen Evaluation von Wortvektoren werden die folgenden Ideen aufgegriffen:\\
  \begin{enumerate}
    \item \textbf{Benchmark-Tests}\\
      Bei dieser pragmatischen Art der Bewertung werden wird das Datenset als Grundlage für
      eine einfach Aufgabe wie Sentiment-Klassifikation oder Part-of-Speech-Tagging verwendet.
      Die Qualität der Daten wird dann anhand der Ergebnisse des Systems gemessen.\\
      Diese extrinische Evaluationsmethode macht ergo nur dann Sinn, wenn man mehr als ein Datenset
      miteinander vergleicht. Dabei muss sichergestellt werden, dass die Tests immer unter den
      selben Bedingungen ablaufen, damit eine Vergleichbarkeit gewährleistet bleibt.\\
    \item \textbf{Wortähnlichkeit}\\
      Hierbei werden Wortpaaren Ähnlichkeitswerte von menschlichen Annotatoren zugeordnet. Anschließend
      werden mit den zu Verfügung stehenden Vektoren Ähnlichkeitswerte für die gleichen Paare berechnet,
      in der Regel mithilfe der Cosinus-Ähnlichkeit. Diese beschreibt die Ähnlichkeit zweier Vektoren
      als den Winkel zwischen ihnen, mit einem Wert von $-1$ ($\hat{=}$ komplett unterschiedlich) über 0
      ($\hat{=}$ orthogonal) und $+1$ ($\hat{=}$ Equivalenz):
      \begin{equation}\label{form:cossim}
        cos(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}
      \end{equation}
      Danach kann mit \emph{Spearman's $\rho$} bzw. \emph{Spearman's rank correlation coefficient}
      anschließend festgestellt werden, ob die beiden Werte für die Wortpaare korrelieren, sprich ob
      das System Paaren, denen von Menschen ein hoher Ähnlichkeitswert zugewiesen wurde auch eine hohe
      Ähnlichkeit zuschreibt. Dabei ist $\rho \in [-1, 1]$ den Grad der Korrelation anzeigt, wobei
      $-1$ einer starken negativen, $+1$ einer starken positiven Korrelation entspricht.
    \item \textbf{Analogien}\\
      Die dritte Methode basiert auf Analogien der Form \emph{a verhält sich zu a* wie b zu b*}.
      Die Daten werden nun dahingehend getestet, indem unter Gebrauch der Cosinus-Ähnlichkeit
      das $b*$ aus dem Vokabular $\mathcal{V}$ gesucht wird, welches besonders ähnlich zu $b$ und $a*$ aber unähnlich zu $a$ ist:
      \begin{equation}
        \underset{\tilde{b}^* \in \mathcal{V}}{argmax}\ cos(\tilde{b}^*, b - a + a^*)
      \end{equation}
      Sind alle Vektoren der Länge eins, so kann diese Gleichung umformuliert werden:
      \begin{equation}
        \underset{\tilde{b}^* \in \mathcal{V}}{argmax}\ cos(\tilde{b}^*, b) - cos(\tilde{b}^*, a) + cos(\tilde{b}^*, a^*)
      \end{equation}
      Diese Methode wird gemeinhin als \textsc{3CosAdd} bezeichnet. (Autor) etablierten dazu jedoch
      eine Alternative namens \textsc{3CosMul}, die bei Tests bessere Ergebnisse produziert:
      \begin{equation}
        \underset{\tilde{b}^* \in \mathcal{V}}{argmax} \frac{cos(\tilde{b}^*, b)\ cos(\tilde{b}^*, a^*)}{cos(\tilde{b}^*, a) + \epsilon}
      \end{equation}
      Dabei ist $\epsilon = 0,001$, um die Divison durch Null zu verhindern.\\
      Der Erfolg der Evaluation kann dann als Anteil der richtig vervollständigten Analogien (bei denen $\tilde{b}^* = b^*$) gemessen werden.
  \end{enumerate}
  In dieser Arbeit sollen die Datensets durch die zweit- und drittgenannte Methode evaluiert werden.
  Ein weiterer Fallstrick liegt allerdings in der Zusammenstellung der Datensets: So liefern
  die genannten nur dann Aussagekräftige Ergebnisse, wenn bei der Wortähnlichkeit die menschlichenen Annotatoren
  zuverlässig und sinnvoll die Paare bewertet haben (zu messen z.B. mit $Cohen's\ \kappa$) und bei
  den Analogien aus der Zusammenstellung ebendieser (welche Entitäten sind enthalten, wie oft kommen diese vor,
  welche semantische Relationen wurden ausgewählt, wurden diese maschinell oder per Hand erzeugt).\\
  Aus diesem Grund sollen die benutzten Evaluationssets im hierauf folgenden Abschnitt \ref{sec:evaldata} näher beleuchtet werden.

  \subsection{Evaluationsdaten}\label{sec:evaldata}

    \subsubsection{Wortpaarähnlichkeit}

    Im Englischen wird für die Wortähnlichkeitsevaluation häufig das \textsc{WordSim353}-Datenset verwendet. Dieses
    wurde unter dem Namen \textsc{Schm280} in deutsche portiert, wobei die Paare nicht nur einfach übersetzt, sondern die
    Ähnlichkeit auch noch von deutschen Muttersprachlern neu bewertet wurde. Es enthält insgesamt 280 Wortpaare.\\ \\
    \textsc{Wortpaare65}, \textsc{Wortpaare222} und \textsc{Wortpaare350} entstammen der Arbeit von (\cite{rubenstein1965contextual}). Dabei werden Wortpaaren
    Werte von 0 ($\hat{=}$ vollkommen unzusammenhängend) bis 4 ($\hat{=}$ stark zusammenhängend) bewertet. Die Anzahl
    der menschlichen Annotatoren sowie deren Übereinstimmung sind in Abbildung \ref{fig:evalsets} festgehalten.

    \begin{figure}[h]
      \centering
      \begin{tabular}{l|cc}
        Datenset & \#Annotatoren & $\kappa$ \\
        \hline
        \textsc{Wortpaare65} & 24 & 0,81 \\
        \textsc{Wortpaare222} & 21 & 0,49 \\
        \textsc{Wortpaare350} & 8 & 0,69 \\
      \end{tabular}
      \caption{Anzahl der Annotatoren und Agreement (als $Cohen's\ \kappa$) der \textsc{Wortpaar}-Evaluationsdatensets.
      \label{fig:evalsets}}
    \end{figure}

    \subsubsection{Analogien}

    Die \textsc{Google semantic/syntactic analogy datasets} wurden von (\cite{mikolov2013efficient}) eingefügt und bestehen
    aus Analogien der Form \emph{a verhält sich zu a* wie b zu b*}. (\cite{koper2015multilingual}) haben diese manuell übersetzt und durch
    drei menschliche Prüfer validieren lassen. Dabei wurde die Kategorie ``adjektiv - adverb'' fallengelassen, da sie
    im Deutschen nicht existiert, weshalb 18.552 Analogien übrigbleiben. Diese werden im Folgenden einfach als
    \textsc{Google} bezeichnet.\\ \\
    \textsc{SemRel} wurde aus Synonomie-, Antonomie- und Hypernomie-Beziehungen von (\cite{koper2015multilingual}) für das Deutsche und Englische
    konstruiert. Dabei werden Substantive, Verben und Adjektive berücksichtigt. In der deutschen Variante sind 2.462 (recht schwierige) Analogien enthalten,
    die aus teilweise sehr seltenen Wörter kreiert wurden.

  \subsection{Evaluationsergebnisse}

  Im Folgenden sollen die Ergebnisse der Wortvektor-Evaluation diskutiert werden. Am Ende dieses Abschnitts werden dabei
  nur die Datensets mit den besten Ergebnissen und deren Konfiguration von Parametern vorgstellt, eine ausführliche
  Übersicht findet sich jedoch im Appendix \ref{AppendixB}.\\

%Mark I     & \textbf{-0,8096}         & \textbf{-0,4640$_{(13)}$}   & \textbf{-0,7302$_{(13)}$}   & \textbf{-0,7094$_{(2)}$}  & \textbf{44,56}  & 1,71 \\
%Mark XIII  & \textbf{-0,8247$_{(1)}$} & \textbf{-0,5066$_{(102)}$}  & \textbf{-0,7494$_{(132)}$}  & -0,7097$_{(48)}$          & 15,51           & \textbf{3,01} \\
%Mark XIV   & -0,8106$_{(1)}$          & -0,4953$_{(102)}$           & -0,7362$_{(132)}$           & \textbf{-0,7205$_{(48)}$} & 14,51           & 2,56 \\

  \begin{figure}[h]
    \centering

    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l||l|l|l|l}
      \textsc{Datenset} & \textsc{Wortpaare65} & \textsc{Wortpaare222} & \textsc{Wortpaare350} & \textsc{Schm280} \\
      \hline
      Mark I & \textbf{-0,8096} & \textbf{-0,4640$_{(13)}$} & \textbf{-0,7302$_{(13)}$} & \textbf{-0,7094$_{(2)}$} \\
      Mark XIII & \textbf{-0,8247$_{(1)}$} & \textbf{-0,5066$_{(102)}$}  & \textbf{-0,7494$_{(132)}$} & -0,7097$_{(48)}$ \\
      Mark XIV & -0,8106$_{(1)}$ & -0,4953$_{(102)}$ & -0,7362$_{(132)}$ & \textbf{-0,7205$_{(48)}$} \\
    \end{tabular}%
    }

    \vspace{0.5cm}

    \begin{tabular}{l||c|c}
      \textsc{Datenset} & \textsc{Google} & \textsc{SemRel} \\
      \hline
      Mark I & \textbf{44,56}  & 1,71 \\
      Mark XIII & 15,51 & \textbf{3,01} \\
      Mark XIV & 14,51 & 2,56 \\
    \end{tabular}

    \caption[Evaluationsergebnisse der besten Vektordatensets]{Evaluationsergebnisse der fünf besten Vektordatensetz. Oben:
    Spearman's $\rho$ für die Wortähnlichkeit mit von menschen gewerteten Wortpaaren. Unten: Treffer beim Vervollständigen
    von Analogien in Prozent. Nicht gefundene Wortvektoren klein in Klammern hinter dem Wert. Für eine vollständige Liste
    der Trainingsparameter jedes Datensets siehe \ref{AppendixA}. Für die gesamte Liste aller Ergebnisse siehe \ref{AppendixB}.}
  \end{figure}

  Bei der Evaluation war zuallererst festzustellen, dass sowohl bei Datensets, die auf dem normalen oder lemmatisierten Korpus
  trainiert wurden, Ergebnisse mit steigender Downsamplingrate schlechter wurden. Dies war sowohl bei Training mit dem CBOW- als
  auch mit dem Skip-Gram-Modell feststellbar. Darüber hinaus lieferte Letzteres im Schnitt bessere Resultate (und benötigte
  während des Trainings auch weniger Zeit). Das Lemmatisieren führte dazu, das während der Bewertung der Sets oft einige
  Schritte nicht durchgeführt werden könnten, was anhand der in Subskript in Klammern stehenden Zahlen abzulesen ist.
  Darunter leidet leider auch wenig die Vergleichbarkeit; es erscheint aber zumindest logisch, dass bei gleichem Evaluationswert
  einem Datenset mit weniger nicht gefundenen Wortvektoren eine höhere Qualität beizumessen ist.\\
  Bei den Analogie-Datensets sind ähnliche Tendenzen sichtbar: Beim \textsc{Google}-Datenset sind die Diskrepanzen jedoch
  deutlich stärker und Vektoren des unlemmatisierten Korpus schneiden deutlich besser ab, was vor allem dadurch
  zu erklären ist, dass in diesem Set auch Flektion geprüft wird (bspw. \emph{schreiben} verhält sich zu \emph{schreibt} wie
   \emph{sagen} zu \emph{sagt}) und diese Formen durch die Lemmatisierung velorengehen. Beim \textsc{SemRel}-Datenset
  verhält sich das ganze allerdings genau umgekehrt, wenn auch die Unterschiede wesentlich feiner sind.\\

  Es ist ferner zu erkennen, dass einige der extern bereitgestellten Evaluationsdaten nicht sehr gut zusammengestellt wurden.
  Bei \textsc{Wortpaare222} prägen sich die Korrelationswerte nicht unter $-0,54$ aus, was dafür spricht, das die Ähnlichkeit
  der Wortpaare für sowohl für das System als möglicherweise auch für die menschlichen Annotatoren schwer einzuschätzen war
  \footnote{Beispielhaft seien hier einige Wortpaare aus \textsc{Wortpaare222} genannt, die nach Ähnlichkeit bewertet werden
  mussten, um das Problem zu illustrieren:\\
  \emph{wahrnehmen} - \emph{Grundsatzfrage} / \emph{groß} - \emph{Arbeitszeitregelung} /
  \emph{Hubschraubertyp} - \emph{einschließlich} / \emph{Büroequipment} - \emph{Institut}.}
  Beim \textsc{SemRel}-Analogienset sticht dieser Fakt noch viel drastischer heraus: Im besten Fall wurden $3\%$ (sic!)
  der Analogien richtig vom System vervollständigt. Es sei angemerkt, dass es auch dem Autor und anderen gefragten Personen
  schwerfiel, stichprobeartig ausgewählte Fragen richtig zu beantworten
  \footnote{\emph{Lieblichkeit} verhält sich zu \emph{Anmut} wie \emph{Mittelklasse} zu$\ldots$? \rotatebox[origin=c]{180}{\emph{Mittelschicht}}\\
  \emph{Zivilgesellschaft} verhält sich zu \emph{Bürgergesellschaft} wie \emph{Gegenargument} zu$\ldots$? \rotatebox[origin=c]{180}{\emph{Widerspruch}}\\
  \emph{Trio} verhält sich zu \emph{Solo}  wie \emph{Arzt} zu$\ldots$? \rotatebox[origin=c]{180}{\emph{Patient}}}.
  Deshalb stellen sich die Ergebnisse bei diesem Vergleichset ohne klare Tendenz in den Resultaten und generell sehr schlecht dar.\\

  Für die nachfolgenden Schritte wurde die bestabschneidenen Wortvektorsets, namentlich Mark I, XIII und XIV ausgewählt.
